# Unit II â€“ XOR Classification using MLP

## ğŸ“Œ Project Overview
This project demonstrates the use of a **Multilayer Perceptron (MLP)** to classify the XOR logic function. XOR is a **classic non-linearly separable problem**, which cannot be solved by a simple perceptron. The MLP uses a **hidden layer with sigmoid activation** and backpropagation to learn the XOR mapping.

---

## ğŸ¯ Objective
- Train an MLP to classify XOR inputs correctly.
- Visualize the **learning curve** and **decision boundary**.
- Compare MLP performance with a simple perceptron.

---

## ğŸ›  Methodology
1. **Dataset**: XOR truth table  
   | Input 1 | Input 2 | XOR Output |
   |---------|---------|------------|
   | 0       | 0       | 0          |
   | 0       | 1       | 1          |
   | 1       | 0       | 1          |
   | 1       | 1       | 0          |

2. **MLP Architecture**:
   - Input layer: 2 neurons  
   - Hidden layer: 2 neurons (sigmoid activation)  
   - Output layer: 1 neuron (sigmoid activation)  
   - Loss: Mean Squared Error (MSE)  
   - Optimizer: Gradient Descent  

3. **Training**:
   - Forward propagation â†’ compute output  
   - Compute loss  
   - Backpropagation â†’ update weights and biases  
   - Repeat for 10,000 epochs  

---

## ğŸ“Š Results
- **Accuracy**: 100% on XOR dataset  
- **Learning Curve**: Shows MSE decreasing steadily  
- **Decision Boundary**: MLP successfully separates XOR classes, unlike a single-layer perceptron  

### Comparison to Perceptron

| Feature             | Perceptron       | MLP                  |
|--------------------|----------------|--------------------|
| Linear Separation  | âœ… Only linear  | âŒ Linear & Non-linear |
| Hidden Layers      | âŒ None         | âœ… One or more       |
| XOR Classification | âŒ Fails        | âœ… Succeeds          |
| Complexity         | Low             | Moderate             |

---

## ğŸ“ Key Takeaways
- MLPs can solve **non-linearly separable problems**.  
- Hidden layers enable **non-linear transformations** and complex decision boundaries.  
- This project illustrates the **power of backpropagation** in training neural networks.  

---

## ğŸ“Œ Deliverables
1. XOR dataset table  
2. MLP training code with backpropagation  
3. Learning curve plot  
4. Decision boundary plot  
5. Accuracy metric  

